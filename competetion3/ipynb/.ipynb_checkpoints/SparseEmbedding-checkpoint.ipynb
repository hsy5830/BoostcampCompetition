{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6fd878",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6d29daee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T05:53:52.850946Z",
     "start_time": "2022-12-27T05:53:52.827319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.08734223, 0.        ,\n",
       "        0.11484453, 0.        , 0.        , 0.22968907, 0.22968907,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.3445336 , 0.22968907, 0.11484453,\n",
       "        0.22968907, 0.11484453, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.11484453, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.11484453, 0.        , 0.17468446, 0.        ,\n",
       "        0.        , 0.11484453, 0.11484453, 0.        , 0.        ,\n",
       "        0.11484453, 0.11484453, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.08734223, 0.11484453, 0.        , 0.11484453,\n",
       "        0.        , 0.        , 0.11484453, 0.        , 0.        ,\n",
       "        0.        , 0.11484453, 0.11484453, 0.        , 0.        ,\n",
       "        0.11484453, 0.        , 0.22968907, 0.11484453, 0.11484453,\n",
       "        0.11484453, 0.11484453, 0.        , 0.        , 0.        ,\n",
       "        0.11484453, 0.11484453, 0.11484453, 0.        , 0.        ,\n",
       "        0.11484453, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.11484453, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.11484453, 0.11484453, 0.11484453, 0.11484453,\n",
       "        0.11484453, 0.22968907, 0.11484453, 0.11484453, 0.11484453,\n",
       "        0.11484453, 0.11484453, 0.06782903, 0.        , 0.11484453,\n",
       "        0.11484453, 0.11484453, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11484453, 0.11484453,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.19464203, 0.19464203, 0.19464203, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19464203, 0.19464203, 0.19464203,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.19464203,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.19464203,\n",
       "        0.        , 0.        , 0.19464203, 0.19464203, 0.19464203,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.19464203, 0.        , 0.        , 0.19464203, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.19464203, 0.        ,\n",
       "        0.        , 0.19464203, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.19464203, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.19464203,\n",
       "        0.19464203, 0.19464203, 0.        , 0.19464203, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.19464203, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.22991743, 0.19464203, 0.        ,\n",
       "        0.        , 0.        , 0.19464203, 0.        , 0.19464203,\n",
       "        0.        , 0.19464203, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.14273481, 0.14273481, 0.14273481, 0.1085535 , 0.14273481,\n",
       "        0.        , 0.14273481, 0.14273481, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14273481, 0.14273481,\n",
       "        0.14273481, 0.14273481, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.14273481, 0.        , 0.14273481, 0.14273481, 0.        ,\n",
       "        0.14273481, 0.        , 0.14273481, 0.1085535 , 0.14273481,\n",
       "        0.14273481, 0.        , 0.        , 0.14273481, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.14273481, 0.1085535 , 0.        , 0.14273481, 0.        ,\n",
       "        0.        , 0.14273481, 0.        , 0.        , 0.14273481,\n",
       "        0.14273481, 0.        , 0.        , 0.        , 0.28546963,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.14273481, 0.14273481, 0.14273481,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.14273481,\n",
       "        0.        , 0.14273481, 0.14273481, 0.14273481, 0.        ,\n",
       "        0.        , 0.        , 0.14273481, 0.        , 0.14273481,\n",
       "        0.        , 0.28546963, 0.14273481, 0.14273481, 0.14273481,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.08430148, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14273481, 0.        ,\n",
       "        0.14273481, 0.        , 0.14273481, 0.        , 0.        ,\n",
       "        0.14273481]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"세계 배달 피자 리더 도미노피자가 우리 고구마를 활용한 신메뉴를 출시한다.도미노피자는 오는 2월 1일 국내산 고구마와 4가지 치즈가 어우러진 신메뉴 `우리 고구마 피자`를 출시하고 전 매장에서 판매를 시작한다. 이번에 도미노피자가 내놓은 신메뉴 `우리 고구마 피자`는 까다롭게 엄선한 국내산 고구마를 무스와 큐브 형태로 듬뿍 올리고, 모차렐라, 카망베르, 체더 치즈와 리코타 치즈 소스 등 4가지 치즈와 와규 크럼블을 더한 프리미엄 고구마 피자다.\",\n",
    "    \"피자의 발상지이자 원조라고 할 수 있는 남부의 나폴리식 피자(Pizza Napolitana)는 재료 본연의 맛에 집중하여 뛰어난 식감을 자랑한다. 대표적인 나폴리 피자로는 피자 마리나라(Pizza Marinara)와 피자 마르게리타(Pizza Margherita)가 있다.\",\n",
    "    \"도미노피자가 삼일절을 맞아 '방문포장 1+1' 이벤트를 진행한다. 이번 이벤트는 도미노피자 102개 매장에서 3월 1일 단 하루 동안 방문포장 온라인, 오프라인 주문 시 피자 1판을 더 증정하는 이벤트다. 온라인 주문 시 장바구니에 2판을 담은 후 할인 적용이 가능하며, 동일 가격 또는 낮은 가격의 피자를 고객이 선택하면 무료로 증정한다.\"\n",
    "]\n",
    "\n",
    "def tokenizer(sent):\n",
    "    return sent.split(\" \")\n",
    "\n",
    "tokenized_corpus = [tokenizer(doc) for doc in corpus]\n",
    "\n",
    "\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(\n",
    "    tokenizer=tokenizer, #ngram_range=(1, 2), max_features=50000,\n",
    ")\n",
    "\n",
    "p_embedding = tfidf_vec.fit_transform(corpus)\n",
    "p_embedding.toarray()\n",
    "# p_embedding.shape\n",
    "# tf_idf = tfidf_vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f795a616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T06:42:15.200452Z",
     "start_time": "2022-12-27T06:42:15.188958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bee891ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T06:42:47.992682Z",
     "start_time": "2022-12-27T06:42:47.983362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 116)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_embedding.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b478fdb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T05:43:59.063607Z",
     "start_time": "2022-12-27T05:43:59.054657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 116)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0a6f155b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T05:51:27.375829Z",
     "start_time": "2022-12-27T05:51:27.363738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1624147 , 0.        , 0.10092875]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"도미노피자 신메뉴\"\n",
    "query_vec = tfidf_vec.transform([query])\n",
    "\n",
    "result = query_vec * p_embedding.T\n",
    "result.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99617b17",
   "metadata": {},
   "source": [
    "## retrieval.py (TF-IDF ver.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66deaaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T08:19:18.060407Z",
     "start_time": "2022-12-27T08:19:17.383482Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import List, NoReturn, Optional, Tuple, Union\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class SparseRetrieval:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenize_fn,\n",
    "        data_path: Optional[str] = \"../data/\",\n",
    "        context_path: Optional[str] = \"wikipedia_documents.json\",\n",
    "    ) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tokenize_fn:\n",
    "                기본 text를 tokenize해주는 함수입니다.\n",
    "                아래와 같은 함수들을 사용할 수 있습니다.\n",
    "                - lambda x: x.split(' ')\n",
    "                - Huggingface Tokenizer\n",
    "                - konlpy.tag의 Mecab\n",
    "\n",
    "            data_path:\n",
    "                데이터가 보관되어 있는 경로입니다.\n",
    "\n",
    "            context_path:\n",
    "                Passage들이 묶여있는 파일명입니다.\n",
    "\n",
    "            data_path/context_path가 존재해야합니다.\n",
    "\n",
    "        Summary:\n",
    "            Passage 파일을 불러오고 TfidfVectorizer를 선언하는 기능을 합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "\n",
    "        self.contexts = list(\n",
    "            dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    "        )  # set 은 매번 순서가 바뀌므로\n",
    "        \n",
    "        print(f\"Lengths of unique contexts : {len(self.contexts)}\")\n",
    "        self.ids = list(range(len(self.contexts)))\n",
    "\n",
    "        # Transform by vectorizer\n",
    "        self.tfidfv = TfidfVectorizer(\n",
    "            tokenizer=tokenize_fn, ngram_range=(1, 2), max_features=50000,\n",
    "        )\n",
    "\n",
    "        self.p_embedding = None  # get_sparse_embedding()로 생성합니다\n",
    "        self.indexer = None  # build_faiss()로 생성합니다.\n",
    "\n",
    "    def get_sparse_embedding(self) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Summary:\n",
    "            Passage Embedding을 만들고\n",
    "            TFIDF와 Embedding을 pickle로 저장합니다.\n",
    "            만약 미리 저장된 파일이 있으면 저장된 pickle을 불러옵니다.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pickle을 저장합니다.\n",
    "        pickle_name = f\"sparse_embedding.bin\"\n",
    "        tfidfv_name = f\"tfidv.bin\"\n",
    "        emd_path = os.path.join(self.data_path, pickle_name)\n",
    "        tfidfv_path = os.path.join(self.data_path, tfidfv_name)\n",
    "\n",
    "        if os.path.isfile(emd_path) and os.path.isfile(tfidfv_path): # 파일 있으면 pickle 불러오기\n",
    "            with open(emd_path, \"rb\") as file:\n",
    "                self.p_embedding = pickle.load(file)\n",
    "            with open(tfidfv_path, \"rb\") as file:\n",
    "                self.tfidfv = pickle.load(file)\n",
    "            print(\"Embedding pickle load.\")\n",
    "        else: # 없으면 pickle 저장\n",
    "            print(\"Build passage embedding\")\n",
    "            self.p_embedding = self.tfidfv.fit_transform(self.contexts)\n",
    "            print(self.p_embedding.shape)\n",
    "            with open(emd_path, \"wb\") as file:\n",
    "                pickle.dump(self.p_embedding, file)\n",
    "            with open(tfidfv_path, \"wb\") as file:\n",
    "                pickle.dump(self.tfidfv, file)\n",
    "            print(\"Embedding pickle saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a710583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T08:19:21.103637Z",
     "start_time": "2022-12-27T08:19:18.063208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/\"\n",
    "context_path = \"wikipedia_documents.json\"\n",
    "model_name_or_path = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False,)\n",
    "\n",
    "\n",
    "retriever = SparseRetrieval(\n",
    "        tokenize_fn=tokenizer.tokenize,\n",
    "        data_path=data_path,\n",
    "        context_path=context_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3582143",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T08:19:26.345324Z",
     "start_time": "2022-12-27T08:19:21.105601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build passage embedding\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sy/871lxvt55dl474wwk5xc7r1c0000gn/T/com.apple.shortcuts.mac-helper/ipykernel_33879/3502596382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sparse_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/sy/871lxvt55dl474wwk5xc7r1c0000gn/T/com.apple.shortcuts.mac-helper/ipykernel_33879/977180930.py\u001b[0m in \u001b[0;36mget_sparse_embedding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 없으면 pickle 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Build passage embedding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidfv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memd_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/boost0/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \"\"\"\n\u001b[1;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/boost0/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/boost0/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/boost0/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/boost0/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/boost0/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/boost0/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_strip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/boost0/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_run_split_on_punc\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    462\u001b[0m                     \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                 \u001b[0mstart_new_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retriever.get_sparse_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6615a308",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a33fc0",
   "metadata": {},
   "source": [
    "## BM25Okapi 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb6fe91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T05:24:14.538956Z",
     "start_time": "2022-12-27T05:24:11.646032Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/boost0/lib/python3.7/site-packages (from rank_bm25) (1.21.5)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "# # BM25 패키지 설치\n",
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88198b9",
   "metadata": {},
   "source": [
    "## BM25 기반 Embedding 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db75a56a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T11:00:45.008933Z",
     "start_time": "2022-12-27T11:00:44.996425Z"
    }
   },
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = [\n",
    "    \"세계 배달 피자 리더 도미노피자가 우리 고구마를 활용한 신메뉴를 출시한다.도미노피자는 오는 2월 1일 국내산 고구마와 4가지 치즈가 어우러진 신메뉴 `우리 고구마 피자`를 출시하고 전 매장에서 판매를 시작한다. 이번에 도미노피자가 내놓은 신메뉴 `우리 고구마 피자`는 까다롭게 엄선한 국내산 고구마를 무스와 큐브 형태로 듬뿍 올리고, 모차렐라, 카망베르, 체더 치즈와 리코타 치즈 소스 등 4가지 치즈와 와규 크럼블을 더한 프리미엄 고구마 피자다.\",\n",
    "    \"피자의 발상지이자 원조라고 할 수 있는 남부의 나폴리식 피자(Pizza Napolitana)는 재료 본연의 맛에 집중하여 뛰어난 식감을 자랑한다. 대표적인 나폴리 피자로는 피자 마리나라(Pizza Marinara)와 피자 마르게리타(Pizza Margherita)가 있다.\",\n",
    "    \"도미노피자가 삼일절을 맞아 '방문포장 1+1' 이벤트를 진행한다. 이번 이벤트는 도미노피자 102개 매장에서 3월 1일 단 하루 동안 방문포장 온라인, 오프라인 주문 시 피자 1판을 더 증정하는 이벤트다. 온라인 주문 시 장바구니에 2판을 담은 후 할인 적용이 가능하며, 동일 가격 또는 낮은 가격의 피자를 고객이 선택하면 무료로 증정한다.\"\n",
    "]\n",
    "\n",
    "def tokenizer(sent):\n",
    "    return sent.split(\" \")\n",
    "\n",
    "tokenized_corpus = [tokenizer(doc) for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a41b9d0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T05:43:17.993997Z",
     "start_time": "2022-12-27T05:43:17.979804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'세계': 0.5108256237659907,\n",
       " '배달': 0.5108256237659907,\n",
       " '피자': 0.11580621302033972,\n",
       " '리더': 0.5108256237659907,\n",
       " '도미노피자가': 0.11580621302033972,\n",
       " '우리': 0.5108256237659907,\n",
       " '고구마를': 0.5108256237659907,\n",
       " '활용한': 0.5108256237659907,\n",
       " '신메뉴를': 0.5108256237659907,\n",
       " '출시한다.도미노피자는': 0.5108256237659907,\n",
       " '오는': 0.5108256237659907,\n",
       " '2월': 0.5108256237659907,\n",
       " '1일': 0.11580621302033972,\n",
       " '국내산': 0.5108256237659907,\n",
       " '고구마와': 0.5108256237659907,\n",
       " '4가지': 0.5108256237659907,\n",
       " '치즈가': 0.5108256237659907,\n",
       " '어우러진': 0.5108256237659907,\n",
       " '신메뉴': 0.5108256237659907,\n",
       " '`우리': 0.5108256237659907,\n",
       " '고구마': 0.5108256237659907,\n",
       " '피자`를': 0.5108256237659907,\n",
       " '출시하고': 0.5108256237659907,\n",
       " '전': 0.5108256237659907,\n",
       " '매장에서': 0.11580621302033972,\n",
       " '판매를': 0.5108256237659907,\n",
       " '시작한다.': 0.5108256237659907,\n",
       " '이번에': 0.5108256237659907,\n",
       " '내놓은': 0.5108256237659907,\n",
       " '피자`는': 0.5108256237659907,\n",
       " '까다롭게': 0.5108256237659907,\n",
       " '엄선한': 0.5108256237659907,\n",
       " '무스와': 0.5108256237659907,\n",
       " '큐브': 0.5108256237659907,\n",
       " '형태로': 0.5108256237659907,\n",
       " '듬뿍': 0.5108256237659907,\n",
       " '올리고,': 0.5108256237659907,\n",
       " '모차렐라,': 0.5108256237659907,\n",
       " '카망베르,': 0.5108256237659907,\n",
       " '체더': 0.5108256237659907,\n",
       " '치즈와': 0.5108256237659907,\n",
       " '리코타': 0.5108256237659907,\n",
       " '치즈': 0.5108256237659907,\n",
       " '소스': 0.5108256237659907,\n",
       " '등': 0.5108256237659907,\n",
       " '와규': 0.5108256237659907,\n",
       " '크럼블을': 0.5108256237659907,\n",
       " '더한': 0.5108256237659907,\n",
       " '프리미엄': 0.5108256237659907,\n",
       " '피자다.': 0.5108256237659907,\n",
       " '피자의': 0.5108256237659907,\n",
       " '발상지이자': 0.5108256237659907,\n",
       " '원조라고': 0.5108256237659907,\n",
       " '할': 0.5108256237659907,\n",
       " '수': 0.5108256237659907,\n",
       " '있는': 0.5108256237659907,\n",
       " '남부의': 0.5108256237659907,\n",
       " '나폴리식': 0.5108256237659907,\n",
       " '피자(Pizza': 0.5108256237659907,\n",
       " 'Napolitana)는': 0.5108256237659907,\n",
       " '재료': 0.5108256237659907,\n",
       " '본연의': 0.5108256237659907,\n",
       " '맛에': 0.5108256237659907,\n",
       " '집중하여': 0.5108256237659907,\n",
       " '뛰어난': 0.5108256237659907,\n",
       " '식감을': 0.5108256237659907,\n",
       " '자랑한다.': 0.5108256237659907,\n",
       " '대표적인': 0.5108256237659907,\n",
       " '나폴리': 0.5108256237659907,\n",
       " '피자로는': 0.5108256237659907,\n",
       " '마리나라(Pizza': 0.5108256237659907,\n",
       " 'Marinara)와': 0.5108256237659907,\n",
       " '마르게리타(Pizza': 0.5108256237659907,\n",
       " 'Margherita)가': 0.5108256237659907,\n",
       " '있다.': 0.5108256237659907,\n",
       " '삼일절을': 0.5108256237659907,\n",
       " '맞아': 0.5108256237659907,\n",
       " \"'방문포장\": 0.5108256237659907,\n",
       " \"1+1'\": 0.5108256237659907,\n",
       " '이벤트를': 0.5108256237659907,\n",
       " '진행한다.': 0.5108256237659907,\n",
       " '이번': 0.5108256237659907,\n",
       " '이벤트는': 0.5108256237659907,\n",
       " '도미노피자': 0.5108256237659907,\n",
       " '102개': 0.5108256237659907,\n",
       " '3월': 0.5108256237659907,\n",
       " '단': 0.5108256237659907,\n",
       " '하루': 0.5108256237659907,\n",
       " '동안': 0.5108256237659907,\n",
       " '방문포장': 0.5108256237659907,\n",
       " '온라인,': 0.5108256237659907,\n",
       " '오프라인': 0.5108256237659907,\n",
       " '주문': 0.5108256237659907,\n",
       " '시': 0.5108256237659907,\n",
       " '1판을': 0.5108256237659907,\n",
       " '더': 0.5108256237659907,\n",
       " '증정하는': 0.5108256237659907,\n",
       " '이벤트다.': 0.5108256237659907,\n",
       " '온라인': 0.5108256237659907,\n",
       " '장바구니에': 0.5108256237659907,\n",
       " '2판을': 0.5108256237659907,\n",
       " '담은': 0.5108256237659907,\n",
       " '후': 0.5108256237659907,\n",
       " '할인': 0.5108256237659907,\n",
       " '적용이': 0.5108256237659907,\n",
       " '가능하며,': 0.5108256237659907,\n",
       " '동일': 0.5108256237659907,\n",
       " '가격': 0.5108256237659907,\n",
       " '또는': 0.5108256237659907,\n",
       " '낮은': 0.5108256237659907,\n",
       " '가격의': 0.5108256237659907,\n",
       " '피자를': 0.5108256237659907,\n",
       " '고객이': 0.5108256237659907,\n",
       " '선택하면': 0.5108256237659907,\n",
       " '무료로': 0.5108256237659907,\n",
       " '증정한다.': 0.5108256237659907}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0a0a086",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T05:25:19.147911Z",
     "start_time": "2022-12-27T05:25:19.141585Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59, 27, 47]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2802809d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T05:40:15.502995Z",
     "start_time": "2022-12-27T05:40:15.487645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'세계': 1,\n",
       "  '배달': 1,\n",
       "  '피자': 1,\n",
       "  '리더': 1,\n",
       "  '도미노피자가': 2,\n",
       "  '우리': 1,\n",
       "  '고구마를': 2,\n",
       "  '활용한': 1,\n",
       "  '신메뉴를': 1,\n",
       "  '출시한다.도미노피자는': 1,\n",
       "  '오는': 1,\n",
       "  '2월': 1,\n",
       "  '1일': 1,\n",
       "  '국내산': 2,\n",
       "  '고구마와': 1,\n",
       "  '4가지': 2,\n",
       "  '치즈가': 1,\n",
       "  '어우러진': 1,\n",
       "  '신메뉴': 2,\n",
       "  '`우리': 2,\n",
       "  '고구마': 3,\n",
       "  '피자`를': 1,\n",
       "  '출시하고': 1,\n",
       "  '전': 1,\n",
       "  '매장에서': 1,\n",
       "  '판매를': 1,\n",
       "  '시작한다.': 1,\n",
       "  '이번에': 1,\n",
       "  '내놓은': 1,\n",
       "  '피자`는': 1,\n",
       "  '까다롭게': 1,\n",
       "  '엄선한': 1,\n",
       "  '무스와': 1,\n",
       "  '큐브': 1,\n",
       "  '형태로': 1,\n",
       "  '듬뿍': 1,\n",
       "  '올리고,': 1,\n",
       "  '모차렐라,': 1,\n",
       "  '카망베르,': 1,\n",
       "  '체더': 1,\n",
       "  '치즈와': 2,\n",
       "  '리코타': 1,\n",
       "  '치즈': 1,\n",
       "  '소스': 1,\n",
       "  '등': 1,\n",
       "  '와규': 1,\n",
       "  '크럼블을': 1,\n",
       "  '더한': 1,\n",
       "  '프리미엄': 1,\n",
       "  '피자다.': 1},\n",
       " {'피자의': 1,\n",
       "  '발상지이자': 1,\n",
       "  '원조라고': 1,\n",
       "  '할': 1,\n",
       "  '수': 1,\n",
       "  '있는': 1,\n",
       "  '남부의': 1,\n",
       "  '나폴리식': 1,\n",
       "  '피자(Pizza': 1,\n",
       "  'Napolitana)는': 1,\n",
       "  '재료': 1,\n",
       "  '본연의': 1,\n",
       "  '맛에': 1,\n",
       "  '집중하여': 1,\n",
       "  '뛰어난': 1,\n",
       "  '식감을': 1,\n",
       "  '자랑한다.': 1,\n",
       "  '대표적인': 1,\n",
       "  '나폴리': 1,\n",
       "  '피자로는': 1,\n",
       "  '피자': 2,\n",
       "  '마리나라(Pizza': 1,\n",
       "  'Marinara)와': 1,\n",
       "  '마르게리타(Pizza': 1,\n",
       "  'Margherita)가': 1,\n",
       "  '있다.': 1},\n",
       " {'도미노피자가': 1,\n",
       "  '삼일절을': 1,\n",
       "  '맞아': 1,\n",
       "  \"'방문포장\": 1,\n",
       "  \"1+1'\": 1,\n",
       "  '이벤트를': 1,\n",
       "  '진행한다.': 1,\n",
       "  '이번': 1,\n",
       "  '이벤트는': 1,\n",
       "  '도미노피자': 1,\n",
       "  '102개': 1,\n",
       "  '매장에서': 1,\n",
       "  '3월': 1,\n",
       "  '1일': 1,\n",
       "  '단': 1,\n",
       "  '하루': 1,\n",
       "  '동안': 1,\n",
       "  '방문포장': 1,\n",
       "  '온라인,': 1,\n",
       "  '오프라인': 1,\n",
       "  '주문': 2,\n",
       "  '시': 2,\n",
       "  '피자': 1,\n",
       "  '1판을': 1,\n",
       "  '더': 1,\n",
       "  '증정하는': 1,\n",
       "  '이벤트다.': 1,\n",
       "  '온라인': 1,\n",
       "  '장바구니에': 1,\n",
       "  '2판을': 1,\n",
       "  '담은': 1,\n",
       "  '후': 1,\n",
       "  '할인': 1,\n",
       "  '적용이': 1,\n",
       "  '가능하며,': 1,\n",
       "  '동일': 1,\n",
       "  '가격': 1,\n",
       "  '또는': 1,\n",
       "  '낮은': 1,\n",
       "  '가격의': 1,\n",
       "  '피자를': 1,\n",
       "  '고객이': 1,\n",
       "  '선택하면': 1,\n",
       "  '무료로': 1,\n",
       "  '증정한다.': 1}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.doc_freqs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d87e7e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T05:40:08.606282Z",
     "start_time": "2022-12-27T05:40:08.594062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'세계': 0.5108256237659907,\n",
       " '배달': 0.5108256237659907,\n",
       " '피자': 0.11580621302033972,\n",
       " '리더': 0.5108256237659907,\n",
       " '도미노피자가': 0.11580621302033972,\n",
       " '우리': 0.5108256237659907,\n",
       " '고구마를': 0.5108256237659907,\n",
       " '활용한': 0.5108256237659907,\n",
       " '신메뉴를': 0.5108256237659907,\n",
       " '출시한다.도미노피자는': 0.5108256237659907,\n",
       " '오는': 0.5108256237659907,\n",
       " '2월': 0.5108256237659907,\n",
       " '1일': 0.11580621302033972,\n",
       " '국내산': 0.5108256237659907,\n",
       " '고구마와': 0.5108256237659907,\n",
       " '4가지': 0.5108256237659907,\n",
       " '치즈가': 0.5108256237659907,\n",
       " '어우러진': 0.5108256237659907,\n",
       " '신메뉴': 0.5108256237659907,\n",
       " '`우리': 0.5108256237659907,\n",
       " '고구마': 0.5108256237659907,\n",
       " '피자`를': 0.5108256237659907,\n",
       " '출시하고': 0.5108256237659907,\n",
       " '전': 0.5108256237659907,\n",
       " '매장에서': 0.11580621302033972,\n",
       " '판매를': 0.5108256237659907,\n",
       " '시작한다.': 0.5108256237659907,\n",
       " '이번에': 0.5108256237659907,\n",
       " '내놓은': 0.5108256237659907,\n",
       " '피자`는': 0.5108256237659907,\n",
       " '까다롭게': 0.5108256237659907,\n",
       " '엄선한': 0.5108256237659907,\n",
       " '무스와': 0.5108256237659907,\n",
       " '큐브': 0.5108256237659907,\n",
       " '형태로': 0.5108256237659907,\n",
       " '듬뿍': 0.5108256237659907,\n",
       " '올리고,': 0.5108256237659907,\n",
       " '모차렐라,': 0.5108256237659907,\n",
       " '카망베르,': 0.5108256237659907,\n",
       " '체더': 0.5108256237659907,\n",
       " '치즈와': 0.5108256237659907,\n",
       " '리코타': 0.5108256237659907,\n",
       " '치즈': 0.5108256237659907,\n",
       " '소스': 0.5108256237659907,\n",
       " '등': 0.5108256237659907,\n",
       " '와규': 0.5108256237659907,\n",
       " '크럼블을': 0.5108256237659907,\n",
       " '더한': 0.5108256237659907,\n",
       " '프리미엄': 0.5108256237659907,\n",
       " '피자다.': 0.5108256237659907,\n",
       " '피자의': 0.5108256237659907,\n",
       " '발상지이자': 0.5108256237659907,\n",
       " '원조라고': 0.5108256237659907,\n",
       " '할': 0.5108256237659907,\n",
       " '수': 0.5108256237659907,\n",
       " '있는': 0.5108256237659907,\n",
       " '남부의': 0.5108256237659907,\n",
       " '나폴리식': 0.5108256237659907,\n",
       " '피자(Pizza': 0.5108256237659907,\n",
       " 'Napolitana)는': 0.5108256237659907,\n",
       " '재료': 0.5108256237659907,\n",
       " '본연의': 0.5108256237659907,\n",
       " '맛에': 0.5108256237659907,\n",
       " '집중하여': 0.5108256237659907,\n",
       " '뛰어난': 0.5108256237659907,\n",
       " '식감을': 0.5108256237659907,\n",
       " '자랑한다.': 0.5108256237659907,\n",
       " '대표적인': 0.5108256237659907,\n",
       " '나폴리': 0.5108256237659907,\n",
       " '피자로는': 0.5108256237659907,\n",
       " '마리나라(Pizza': 0.5108256237659907,\n",
       " 'Marinara)와': 0.5108256237659907,\n",
       " '마르게리타(Pizza': 0.5108256237659907,\n",
       " 'Margherita)가': 0.5108256237659907,\n",
       " '있다.': 0.5108256237659907,\n",
       " '삼일절을': 0.5108256237659907,\n",
       " '맞아': 0.5108256237659907,\n",
       " \"'방문포장\": 0.5108256237659907,\n",
       " \"1+1'\": 0.5108256237659907,\n",
       " '이벤트를': 0.5108256237659907,\n",
       " '진행한다.': 0.5108256237659907,\n",
       " '이번': 0.5108256237659907,\n",
       " '이벤트는': 0.5108256237659907,\n",
       " '도미노피자': 0.5108256237659907,\n",
       " '102개': 0.5108256237659907,\n",
       " '3월': 0.5108256237659907,\n",
       " '단': 0.5108256237659907,\n",
       " '하루': 0.5108256237659907,\n",
       " '동안': 0.5108256237659907,\n",
       " '방문포장': 0.5108256237659907,\n",
       " '온라인,': 0.5108256237659907,\n",
       " '오프라인': 0.5108256237659907,\n",
       " '주문': 0.5108256237659907,\n",
       " '시': 0.5108256237659907,\n",
       " '1판을': 0.5108256237659907,\n",
       " '더': 0.5108256237659907,\n",
       " '증정하는': 0.5108256237659907,\n",
       " '이벤트다.': 0.5108256237659907,\n",
       " '온라인': 0.5108256237659907,\n",
       " '장바구니에': 0.5108256237659907,\n",
       " '2판을': 0.5108256237659907,\n",
       " '담은': 0.5108256237659907,\n",
       " '후': 0.5108256237659907,\n",
       " '할인': 0.5108256237659907,\n",
       " '적용이': 0.5108256237659907,\n",
       " '가능하며,': 0.5108256237659907,\n",
       " '동일': 0.5108256237659907,\n",
       " '가격': 0.5108256237659907,\n",
       " '또는': 0.5108256237659907,\n",
       " '낮은': 0.5108256237659907,\n",
       " '가격의': 0.5108256237659907,\n",
       " '피자를': 0.5108256237659907,\n",
       " '고객이': 0.5108256237659907,\n",
       " '선택하면': 0.5108256237659907,\n",
       " '무료로': 0.5108256237659907,\n",
       " '증정한다.': 0.5108256237659907}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1037daa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T05:52:17.511274Z",
     "start_time": "2022-12-27T05:52:17.505047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bm25.idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "da666822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T06:13:37.790879Z",
     "start_time": "2022-12-27T06:13:37.784751Z"
    }
   },
   "outputs": [],
   "source": [
    "nd = bm25._initialize(corpus)\n",
    "bm25._calc_idf(nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6be988ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T06:46:20.577896Z",
     "start_time": "2022-12-27T06:46:20.569323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.corpus_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee703f22",
   "metadata": {},
   "source": [
    "## BM25 embedding matrix 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c9fc02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T08:19:41.308074Z",
     "start_time": "2022-12-27T08:19:41.263123Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from scipy import sparse\n",
    "\n",
    "def tokenizer(sent):\n",
    "    return sent.split(\" \")\n",
    "\n",
    "def get_p_embedding(corpus, tokenizer):\n",
    "    tokenized_corpus = [tokenizer(doc) for doc in corpus]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    emb = list(bm25.idf.keys()) # embedding 차원\n",
    "    embedding_mat = np.array([np.zeros(len(emb))] * len(corpus))\n",
    "    doc_len = np.array(bm25.doc_len) # 각 context의 길이\n",
    "    \n",
    "    for i in range(len(emb)):\n",
    "        word = emb[i]\n",
    "        q_freq = np.array([(doc.get(word) or 0) for doc in bm25.doc_freqs])\n",
    "        score = (bm25.idf.get(word) or 0) * (q_freq * (bm25.k1 + 1) / (q_freq + bm25.k1 * (1 - bm25.b + bm25.b * doc_len / bm25.avgdl)))\n",
    "        for j in range(len(corpus)):\n",
    "            embedding_mat[j][i] = score[j]\n",
    "    \n",
    "    # numpy array to scipy.sparse.csr.csr_matrix\n",
    "    embedding_mat = sparse.csr_matrix(embedding_mat)\n",
    "    \n",
    "    return embedding_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd49fc94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T08:19:43.006459Z",
     "start_time": "2022-12-27T08:19:42.980316Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x116 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 121 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"세계 배달 피자 리더 도미노피자가 우리 고구마를 활용한 신메뉴를 출시한다.도미노피자는 오는 2월 1일 국내산 고구마와 4가지 치즈가 어우러진 신메뉴 `우리 고구마 피자`를 출시하고 전 매장에서 판매를 시작한다. 이번에 도미노피자가 내놓은 신메뉴 `우리 고구마 피자`는 까다롭게 엄선한 국내산 고구마를 무스와 큐브 형태로 듬뿍 올리고, 모차렐라, 카망베르, 체더 치즈와 리코타 치즈 소스 등 4가지 치즈와 와규 크럼블을 더한 프리미엄 고구마 피자다.\",\n",
    "    \"피자의 발상지이자 원조라고 할 수 있는 남부의 나폴리식 피자(Pizza Napolitana)는 재료 본연의 맛에 집중하여 뛰어난 식감을 자랑한다. 대표적인 나폴리 피자로는 피자 마리나라(Pizza Marinara)와 피자 마르게리타(Pizza Margherita)가 있다.\",\n",
    "    \"도미노피자가 삼일절을 맞아 '방문포장 1+1' 이벤트를 진행한다. 이번 이벤트는 도미노피자 102개 매장에서 3월 1일 단 하루 동안 방문포장 온라인, 오프라인 주문 시 피자 1판을 더 증정하는 이벤트다. 온라인 주문 시 장바구니에 2판을 담은 후 할인 적용이 가능하며, 동일 가격 또는 낮은 가격의 피자를 고객이 선택하면 무료로 증정한다.\"\n",
    "]\n",
    "\n",
    "bm25_emb = get_p_embedding(corpus, tokenizer)\n",
    "bm25_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccddc3e",
   "metadata": {},
   "source": [
    "## 계산된 BM25 이용하여 query에 대한 score 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d76693b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T08:19:47.458205Z",
     "start_time": "2022-12-27T08:19:47.449998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65960979, 0.        , 0.49736316])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"도미노피자 신메뉴\"\n",
    "tokenized_query = tokenizer(query)\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90303275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T08:19:49.287603Z",
     "start_time": "2022-12-27T08:19:49.276550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6596097860279299, 0.0, 0.49736316223189436]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_batch_scores(tokenized_query, [0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de92fae9",
   "metadata": {},
   "source": [
    "## retrieval.py (BM25 ver.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d48409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T08:54:57.437936Z",
     "start_time": "2022-12-27T08:54:56.828700Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import List, NoReturn, Optional, Tuple, Union\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from scipy import sparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SparseRetrieval:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenize_fn,\n",
    "        data_path: Optional[str] = \"../data/\",\n",
    "        context_path: Optional[str] = \"wikipedia_documents.json\",\n",
    "    ) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tokenize_fn:\n",
    "                기본 text를 tokenize해주는 함수입니다.\n",
    "                아래와 같은 함수들을 사용할 수 있습니다.\n",
    "                - lambda x: x.split(' ')\n",
    "                - Huggingface Tokenizer\n",
    "                - konlpy.tag의 Mecab\n",
    "\n",
    "            data_path:\n",
    "                데이터가 보관되어 있는 경로입니다.\n",
    "\n",
    "            context_path:\n",
    "                Passage들이 묶여있는 파일명입니다.\n",
    "\n",
    "            data_path/context_path가 존재해야합니다.\n",
    "\n",
    "        Summary:\n",
    "            Passage 파일을 불러오고 TfidfVectorizer를 선언하는 기능을 합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "\n",
    "        self.contexts = list(\n",
    "            dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    "        )  # set 은 매번 순서가 바뀌므로\n",
    "#         self.contexts = [\n",
    "#     \"세계 배달 피자 리더 도미노피자가 우리 고구마를 활용한 신메뉴를 출시한다.도미노피자는 오는 2월 1일 국내산 고구마와 4가지 치즈가 어우러진 신메뉴 `우리 고구마 피자`를 출시하고 전 매장에서 판매를 시작한다. 이번에 도미노피자가 내놓은 신메뉴 `우리 고구마 피자`는 까다롭게 엄선한 국내산 고구마를 무스와 큐브 형태로 듬뿍 올리고, 모차렐라, 카망베르, 체더 치즈와 리코타 치즈 소스 등 4가지 치즈와 와규 크럼블을 더한 프리미엄 고구마 피자다.\",\n",
    "#     \"피자의 발상지이자 원조라고 할 수 있는 남부의 나폴리식 피자(Pizza Napolitana)는 재료 본연의 맛에 집중하여 뛰어난 식감을 자랑한다. 대표적인 나폴리 피자로는 피자 마리나라(Pizza Marinara)와 피자 마르게리타(Pizza Margherita)가 있다.\",\n",
    "#     \"도미노피자가 삼일절을 맞아 '방문포장 1+1' 이벤트를 진행한다. 이번 이벤트는 도미노피자 102개 매장에서 3월 1일 단 하루 동안 방문포장 온라인, 오프라인 주문 시 피자 1판을 더 증정하는 이벤트다. 온라인 주문 시 장바구니에 2판을 담은 후 할인 적용이 가능하며, 동일 가격 또는 낮은 가격의 피자를 고객이 선택하면 무료로 증정한다.\"\n",
    "# ]\n",
    "        print(f\"Lengths of unique contexts : {len(self.contexts)}\")\n",
    "        print(len(self.contexts))\n",
    "        self.ids = list(range(len(self.contexts)))\n",
    "        self.tokenizer = tokenize_fn\n",
    "        \n",
    "\n",
    "        # Transform by vectorizer\n",
    "        ################################################################################\n",
    "        self.tfidfv = TfidfVectorizer(\n",
    "            tokenizer=tokenize_fn, ngram_range=(1, 2), max_features=50000,\n",
    "        )\n",
    "        self.bm25v = BM25Okapi\n",
    "        ################################################################################\n",
    "\n",
    "        self.p_embedding = None  # get_sparse_embedding()로 생성합니다\n",
    "        self.indexer = None  # build_faiss()로 생성합니다.\n",
    "    \n",
    "    def get_p_embedding(self, corpus, tokenizer):\n",
    "\n",
    "        tokenized_corpus = [tokenizer(doc) for doc in corpus]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        emb = list(bm25.idf.keys()) # embedding 차원\n",
    "#         embedding_mat = np.array([np.zeros(len(emb))] * len(corpus))\n",
    "        doc_len = np.array(bm25.doc_len) # 각 context의 길이\n",
    "        \n",
    "        row = np.array([])\n",
    "        col = np.array([])\n",
    "        s = np.array([])\n",
    "        for i in range(len(emb)):\n",
    "            print(f\"##########{ i }##########\")\n",
    "            word = emb[i]\n",
    "            q_freq = np.array([(doc.get(word) or 0) for doc in bm25.doc_freqs])\n",
    "            score = (bm25.idf.get(word) or 0) * (q_freq * (bm25.k1 + 1) / (q_freq + bm25.k1 * (1 - bm25.b + bm25.b * doc_len / bm25.avgdl)))\n",
    "            for j in range(len(corpus)):\n",
    "                row = np.append(row, j)\n",
    "                col = np.append(col, i)\n",
    "                s = np.append(s,score[j])\n",
    "        \n",
    "        for i in range(len(doc_len)):\n",
    "            for j in range(doc_len[i]):\n",
    "                \n",
    "            \n",
    "\n",
    "        # numpy array to scipy.sparse.csr.csr_matrix\n",
    "        embedding_mat = sparse.csr_matrix((s, (row, col)), shape = (len(corpus), len(emb)))\n",
    "        return embedding_mat\n",
    "    \n",
    "    def get_sparse_embedding(self) -> NoReturn:\n",
    "        # corpus가 너무 커서 너무 오래 걸림\n",
    "        \"\"\"\n",
    "        Summary:\n",
    "            Passage Embedding을 만들고\n",
    "            TFIDF와 Embedding을 pickle로 저장합니다.\n",
    "            만약 미리 저장된 파일이 있으면 저장된 pickle을 불러옵니다.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pickle을 저장합니다.\n",
    "        ################################################################################\n",
    "        pickle_name = f\"sparse_embedding.bin\"\n",
    "        tfidfv_name = f\"tfidv.bin\"\n",
    "        bm25v_name = f\"bm25v.bin\"\n",
    "        emd_path = os.path.join(self.data_path, pickle_name)\n",
    "        tfidfv_path = os.path.join(self.data_path, tfidfv_name)\n",
    "        bm25v_path = os.path.join(self.data_path, bm25v_name)\n",
    "        ################################################################################\n",
    "\n",
    "        ################################################################################\n",
    "        if os.path.isfile(emd_path) and os.path.isfile(bm25v_path): # 파일 있으면 pickle 불러오기\n",
    "        ################################################################################\n",
    "            with open(emd_path, \"rb\") as file:\n",
    "                self.p_embedding = pickle.load(file)\n",
    "            ################################################################################\n",
    "            with open(bm25v_path, \"rb\") as file:\n",
    "                self.bm25v = pickle.load(file)\n",
    "            ################################################################################\n",
    "            print(\"Embedding pickle load.\")\n",
    "        else: # 없으면 pickle 저장\n",
    "            print(\"Build passage embedding\")\n",
    "            ################################################################################\n",
    "            self.p_embedding = self.get_p_embedding(self.contexts, self.tokenizer)\n",
    "            print(\"시작\")\n",
    "            ################################################################################\n",
    "            print(self.p_embedding.shape)\n",
    "            with open(emd_path, \"wb\") as file:\n",
    "                pickle.dump(self.p_embedding, file)\n",
    "            ################################################################################    \n",
    "            with open(tfidfv_path, \"wb\") as file:\n",
    "                pickle.dump(self.bm25v, file)\n",
    "            ################################################################################\n",
    "            print(\"Embedding pickle saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68cbe9ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T08:55:02.969725Z",
     "start_time": "2022-12-27T08:55:00.601096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n",
      "56737\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/\"\n",
    "context_path = \"wikipedia_documents.json\"\n",
    "model_name_or_path = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False,)\n",
    "\n",
    "retriever = SparseRetrieval(\n",
    "        tokenize_fn=tokenizer.tokenize,\n",
    "        data_path=data_path,\n",
    "        context_path=context_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0ce93d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T09:14:02.066953Z",
     "start_time": "2022-12-27T08:55:03.422619Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build passage embedding\n",
      "하하하\n",
      "하하하\n",
      "하하하\n",
      "##########0##########\n",
      "##########1##########\n",
      "##########2##########\n",
      "##########3##########\n",
      "##########4##########\n",
      "##########5##########\n",
      "##########6##########\n",
      "##########7##########\n",
      "##########8##########\n",
      "##########9##########\n",
      "##########10##########\n",
      "##########11##########\n",
      "##########12##########\n",
      "##########13##########\n",
      "##########14##########\n",
      "##########15##########\n",
      "##########16##########\n",
      "##########17##########\n",
      "##########18##########\n",
      "##########19##########\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sy/871lxvt55dl474wwk5xc7r1c0000gn/T/com.apple.shortcuts.mac-helper/ipykernel_35044/3502596382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sparse_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/sy/871lxvt55dl474wwk5xc7r1c0000gn/T/com.apple.shortcuts.mac-helper/ipykernel_35044/189347489.py\u001b[0m in \u001b[0;36mget_sparse_embedding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Build passage embedding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_p_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"시작\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/sy/871lxvt55dl474wwk5xc7r1c0000gn/T/com.apple.shortcuts.mac-helper/ipykernel_35044/189347489.py\u001b[0m in \u001b[0;36mget_p_embedding\u001b[0;34m(self, corpus, tokenizer)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# numpy array to scipy.sparse.csr.csr_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retriever.get_sparse_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ec355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus에서 불러오기 :  분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8dfc5",
   "metadata": {},
   "source": [
    "# BM25 - get_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e337a",
   "metadata": {
    "code_folding": [
     70,
     93,
     97,
     135,
     198,
     235,
     237,
     279,
     348,
     371,
     373
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import List, NoReturn, Optional, Tuple, Union\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from scipy import sparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SparseRetrieval:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenize_fn,\n",
    "        data_path: Optional[str] = \"../data/\",\n",
    "        context_path: Optional[str] = \"wikipedia_documents.json\",\n",
    "        emb_type,\n",
    "    ) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tokenize_fn:\n",
    "                기본 text를 tokenize해주는 함수입니다.\n",
    "                아래와 같은 함수들을 사용할 수 있습니다.\n",
    "                - lambda x: x.split(' ')\n",
    "                - Huggingface Tokenizer\n",
    "                - konlpy.tag의 Mecab\n",
    "\n",
    "            data_path:\n",
    "                데이터가 보관되어 있는 경로입니다.\n",
    "\n",
    "            context_path:\n",
    "                Passage들이 묶여있는 파일명입니다.\n",
    "\n",
    "            data_path/context_path가 존재해야합니다.\n",
    "\n",
    "        Summary:\n",
    "            Passage 파일을 불러오고 TfidfVectorizer를 선언하는 기능을 합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "\n",
    "        self.contexts = list(\n",
    "            dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    "        )  # set 은 매번 순서가 바뀌므로\n",
    "#         self.contexts = [\n",
    "#     \"세계 배달 피자 리더 도미노피자가 우리 고구마를 활용한 신메뉴를 출시한다.도미노피자는 오는 2월 1일 국내산 고구마와 4가지 치즈가 어우러진 신메뉴 `우리 고구마 피자`를 출시하고 전 매장에서 판매를 시작한다. 이번에 도미노피자가 내놓은 신메뉴 `우리 고구마 피자`는 까다롭게 엄선한 국내산 고구마를 무스와 큐브 형태로 듬뿍 올리고, 모차렐라, 카망베르, 체더 치즈와 리코타 치즈 소스 등 4가지 치즈와 와규 크럼블을 더한 프리미엄 고구마 피자다.\",\n",
    "#     \"피자의 발상지이자 원조라고 할 수 있는 남부의 나폴리식 피자(Pizza Napolitana)는 재료 본연의 맛에 집중하여 뛰어난 식감을 자랑한다. 대표적인 나폴리 피자로는 피자 마리나라(Pizza Marinara)와 피자 마르게리타(Pizza Margherita)가 있다.\",\n",
    "#     \"도미노피자가 삼일절을 맞아 '방문포장 1+1' 이벤트를 진행한다. 이번 이벤트는 도미노피자 102개 매장에서 3월 1일 단 하루 동안 방문포장 온라인, 오프라인 주문 시 피자 1판을 더 증정하는 이벤트다. 온라인 주문 시 장바구니에 2판을 담은 후 할인 적용이 가능하며, 동일 가격 또는 낮은 가격의 피자를 고객이 선택하면 무료로 증정한다.\"\n",
    "# ]\n",
    "        print(f\"Lengths of unique contexts : {len(self.contexts)}\")\n",
    "        print(len(self.contexts))\n",
    "        self.ids = list(range(len(self.contexts)))\n",
    "        self.tokenizer = tokenize_fn\n",
    "        self.emb_type = emb_type ###\n",
    "\n",
    "        # Transform by vectorizer\n",
    "        self.tfidfv = TfidfVectorizer(\n",
    "            tokenizer=tokenize_fn, ngram_range=(1, 2), max_features=50000,\n",
    "        )\n",
    "\n",
    "        self.p_embedding = None  # get_sparse_embedding()로 생성합니다\n",
    "        self.indexer = None  # build_faiss()로 생성합니다.\n",
    "    \n",
    "    def get_sparse_embedding(self) -> NoReturn: # for both TF-IDF or BM25\n",
    "        if self.emb_type == 'tfidf':\n",
    "            # Pickle을 저장합니다.\n",
    "            pickle_name = f\"sparse_embedding.bin\"\n",
    "            tfidfv_name = f\"tfidv.bin\"\n",
    "            emd_path = os.path.join(self.data_path, pickle_name)\n",
    "            tfidfv_path = os.path.join(self.data_path, tfidfv_name)\n",
    "\n",
    "            if os.path.isfile(emd_path) and os.path.isfile(tfidfv_path):\n",
    "                with open(emd_path, \"rb\") as file:\n",
    "                    self.p_embedding = pickle.load(file)\n",
    "                with open(tfidfv_path, \"rb\") as file:\n",
    "                    self.tfidfv = pickle.load(file)\n",
    "                print(\"Embedding pickle load.\")\n",
    "            else:\n",
    "                print(\"Build passage embedding\")\n",
    "                self.p_embedding = self.tfidfv.fit_transform(self.contexts)\n",
    "                print(self.p_embedding.shape)\n",
    "                with open(emd_path, \"wb\") as file:\n",
    "                    pickle.dump(self.p_embedding, file)\n",
    "                with open(tfidfv_path, \"wb\") as file:\n",
    "                    pickle.dump(self.tfidfv, file)\n",
    "                print(\"Embedding pickle saved.\")\n",
    "        else:\n",
    "            with timer(\"bm25 building\"):\n",
    "                self.bm25 = BM25Okapi(self.contexts, tokenizer=self.tokenize_fn)\n",
    "            \n",
    "    def build_faiss(self, num_clusters=64) -> NoReturn: # for TF-IDF\n",
    "\n",
    "        \"\"\"\n",
    "        Summary:\n",
    "            속성으로 저장되어 있는 Passage Embedding을\n",
    "            Faiss indexer에 fitting 시켜놓습니다.\n",
    "            이렇게 저장된 indexer는 `get_relevant_doc`에서 유사도를 계산하는데 사용됩니다.\n",
    "\n",
    "        Note:\n",
    "            Faiss는 Build하는데 시간이 오래 걸리기 때문에,\n",
    "            매번 새롭게 build하는 것은 비효율적입니다.\n",
    "            그렇기 때문에 build된 index 파일을 저정하고 다음에 사용할 때 불러옵니다.\n",
    "            다만 이 index 파일은 용량이 1.4Gb+ 이기 때문에 여러 num_clusters로 시험해보고\n",
    "            제일 적절한 것을 제외하고 모두 삭제하는 것을 권장합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        indexer_name = f\"faiss_clusters{num_clusters}.index\"\n",
    "        indexer_path = os.path.join(self.data_path, indexer_name)\n",
    "        if os.path.isfile(indexer_path):\n",
    "            print(\"Load Saved Faiss Indexer.\")\n",
    "            self.indexer = faiss.read_index(indexer_path)\n",
    "\n",
    "        else:\n",
    "            p_emb = self.p_embedding.astype(np.float32).toarray()\n",
    "            emb_dim = p_emb.shape[-1]\n",
    "\n",
    "            num_clusters = num_clusters\n",
    "            quantizer = faiss.IndexFlatL2(emb_dim)\n",
    "\n",
    "            self.indexer = faiss.IndexIVFScalarQuantizer(\n",
    "                quantizer, quantizer.d, num_clusters, faiss.METRIC_L2\n",
    "            )\n",
    "            self.indexer.train(p_emb)\n",
    "            self.indexer.add(p_emb)\n",
    "            faiss.write_index(self.indexer, indexer_path)\n",
    "            print(\"Faiss Indexer Saved.\")\n",
    "            \n",
    "    def retrieve(  # for both TF-IDF or BM25\n",
    "    ) -> Union[Tuple[List, List], pd.DataFrame]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query_or_dataset (Union[str, Dataset]):\n",
    "                str이나 Dataset으로 이루어진 Query를 받습니다.\n",
    "                str 형태인 하나의 query만 받으면 `get_relevant_doc`을 통해 유사도를 구합니다.\n",
    "                Dataset 형태는 query를 포함한 HF.Dataset을 받습니다.\n",
    "                이 경우 `get_relevant_doc_bulk`를 통해 유사도를 구합니다.\n",
    "            topk (Optional[int], optional): Defaults to 1.\n",
    "                상위 몇 개의 passage를 사용할 것인지 지정합니다.\n",
    "\n",
    "        Returns:\n",
    "            1개의 Query를 받는 경우  -> Tuple(List, List)\n",
    "            다수의 Query를 받는 경우 -> pd.DataFrame: [description]\n",
    "\n",
    "        Note:\n",
    "            다수의 Query를 받는 경우,\n",
    "                Ground Truth가 있는 Query (train/valid) -> 기존 Ground Truth Passage를 같이 반환합니다.\n",
    "                Ground Truth가 없는 Query (test) -> Retrieval한 Passage만 반환합니다.\n",
    "        \"\"\"\n",
    "        # 문제가 될 수도 있는 부분\n",
    "        assert self.p_embedding is not None, \"get_sparse_embedding() 메소드를 먼저 수행해줘야합니다.\"\n",
    "\n",
    "        if isinstance(query_or_dataset, str):\n",
    "            doc_scores, doc_indices = self.get_relevant_doc(query_or_dataset, k=topk)\n",
    "            print(\"[Search query]\\n\", query_or_dataset, \"\\n\")\n",
    "\n",
    "            for i in range(topk):\n",
    "                print(f\"Top-{i+1} passage with score {doc_scores[i]:4f}\")\n",
    "                print(self.contexts[doc_indices[i]])\n",
    "\n",
    "            return (doc_scores, [self.contexts[doc_indices[i]] for i in range(topk)])\n",
    "\n",
    "        elif isinstance(query_or_dataset, Dataset):\n",
    "\n",
    "            # Retrieve한 Passage를 pd.DataFrame으로 반환합니다.\n",
    "            total = []\n",
    "            with timer(\"query exhaustive search\"):\n",
    "                doc_scores, doc_indices = self.get_relevant_doc_bulk(\n",
    "                    query_or_dataset[\"question\"], k=topk\n",
    "                )\n",
    "            for idx, example in enumerate(\n",
    "                tqdm(query_or_dataset, desc=\"Sparse retrieval: \")\n",
    "            ):\n",
    "                tmp = {\n",
    "                    # Query와 해당 id를 반환합니다.\n",
    "                    \"question\": example[\"question\"],\n",
    "                    \"id\": example[\"id\"],\n",
    "                    # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "                    \"context\": \" \".join(\n",
    "                        [self.contexts[pid] for pid in doc_indices[idx]]\n",
    "                    ),\n",
    "                }\n",
    "                if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "                    # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "                    tmp[\"original_context\"] = example[\"context\"]\n",
    "                    tmp[\"answers\"] = example[\"answers\"]\n",
    "                total.append(tmp)\n",
    "\n",
    "            cqas = pd.DataFrame(total)\n",
    "            return cqas\n",
    "        \n",
    "    def get_relevant_doc(self, query: str, k: Optional[int] = 1) -> Tuple[List, List]: # for both TF-IDF or BM25\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (str):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "        if self.emb_type == 'tfidf':\n",
    "            with timer(\"transform\"):\n",
    "                query_vec = self.tfidfv.transform([query])\n",
    "            assert (\n",
    "                np.sum(query_vec) != 0\n",
    "            ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "            with timer(\"query ex search\"):\n",
    "                result = query_vec * self.p_embedding.T\n",
    "            if not isinstance(result, np.ndarray):\n",
    "                result = result.toarray()\n",
    "            sorted_result = np.argsort(result.squeeze())[::-1]\n",
    "            doc_score = result.squeeze()[sorted_result].tolist()[:k]\n",
    "            doc_indices = sorted_result.tolist()[:k]\n",
    "            return doc_score, doc_indices \n",
    "        \n",
    "        else: # bm25\n",
    "            with timer(\"transform\"):\n",
    "                tokenized_query = self.tokenize_fn(query)\n",
    "            with timer(\"query ex search\"):\n",
    "                result = self.bm25.get_scores(tokenized_query)\n",
    "            sorted_result = np.argsort(result)[::-1]\n",
    "            doc_score = result[sorted_result].tolist()[:k]\n",
    "            doc_indices = sorted_result.tolist()[:k]\n",
    "            return doc_score, doc_indices\n",
    "\n",
    "    def get_relevant_doc_bulk( # for both TF-IDF or BM25\n",
    "        self, queries: List, k: Optional[int] = 1 # for TF-IDF\n",
    "    ) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            queries (List):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "        if self.emb_type == 'tfidf':\n",
    "            query_vec = self.tfidfv.transform(queries)\n",
    "            assert (\n",
    "                np.sum(query_vec) != 0\n",
    "            ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "            result = query_vec * self.p_embedding.T\n",
    "            if not isinstance(result, np.ndarray):\n",
    "                result = result.toarray()\n",
    "            doc_scores = []\n",
    "            doc_indices = []\n",
    "            for i in range(result.shape[0]):\n",
    "                sorted_result = np.argsort(result[i, :])[::-1]\n",
    "                doc_scores.append(result[i, :][sorted_result].tolist()[:k])\n",
    "                doc_indices.append(sorted_result.tolist()[:k])\n",
    "            return doc_scores, doc_indices\n",
    "        else:\n",
    "            with timer(\"transform\"):\n",
    "                tokenized_queris = [self.tokenize_fn(query) for query in queries]\n",
    "            with timer(\"query ex search\"):\n",
    "                result = np.array([self.bm25.get_scores(tokenized_query) for tokenized_query in tqdm(tokenized_queris)])\n",
    "            doc_scores = []\n",
    "            doc_indices = []\n",
    "            for i in range(result.shape[0]):\n",
    "                sorted_result = np.argsort(result[i, :])[::-1]\n",
    "                doc_scores.append(result[i, :][sorted_result].tolist()[:k])\n",
    "                doc_indices.append(sorted_result.tolist()[:k])\n",
    "            return doc_scores, doc_indices\n",
    "    \n",
    "    def retrieve_faiss(\n",
    "        self, query_or_dataset: Union[str, Dataset], topk: Optional[int] = 1 # for TF-IDF\n",
    "    ) -> Union[Tuple[List, List], pd.DataFrame]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query_or_dataset (Union[str, Dataset]):\n",
    "                str이나 Dataset으로 이루어진 Query를 받습니다.\n",
    "                str 형태인 하나의 query만 받으면 `get_relevant_doc`을 통해 유사도를 구합니다.\n",
    "                Dataset 형태는 query를 포함한 HF.Dataset을 받습니다.\n",
    "                이 경우 `get_relevant_doc_bulk`를 통해 유사도를 구합니다.\n",
    "            topk (Optional[int], optional): Defaults to 1.\n",
    "                상위 몇 개의 passage를 사용할 것인지 지정합니다.\n",
    "\n",
    "        Returns:\n",
    "            1개의 Query를 받는 경우  -> Tuple(List, List)\n",
    "            다수의 Query를 받는 경우 -> pd.DataFrame: [description]\n",
    "\n",
    "        Note:\n",
    "            다수의 Query를 받는 경우,\n",
    "                Ground Truth가 있는 Query (train/valid) -> 기존 Ground Truth Passage를 같이 반환합니다.\n",
    "                Ground Truth가 없는 Query (test) -> Retrieval한 Passage만 반환합니다.\n",
    "            retrieve와 같은 기능을 하지만 faiss.indexer를 사용합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.indexer is not None, \"build_faiss()를 먼저 수행해주세요.\"\n",
    "\n",
    "        if isinstance(query_or_dataset, str):\n",
    "            doc_scores, doc_indices = self.get_relevant_doc_faiss(\n",
    "                query_or_dataset, k=topk\n",
    "            )\n",
    "            print(\"[Search query]\\n\", query_or_dataset, \"\\n\")\n",
    "\n",
    "            for i in range(topk):\n",
    "                print(\"Top-%d passage with score %.4f\" % (i + 1, doc_scores[i]))\n",
    "                print(self.contexts[doc_indices[i]])\n",
    "\n",
    "            return (doc_scores, [self.contexts[doc_indices[i]] for i in range(topk)])\n",
    "\n",
    "        elif isinstance(query_or_dataset, Dataset):\n",
    "\n",
    "            # Retrieve한 Passage를 pd.DataFrame으로 반환합니다.\n",
    "            queries = query_or_dataset[\"question\"]\n",
    "            total = []\n",
    "\n",
    "            with timer(\"query faiss search\"):\n",
    "                doc_scores, doc_indices = self.get_relevant_doc_bulk_faiss(\n",
    "                    queries, k=topk\n",
    "                )\n",
    "            for idx, example in enumerate(\n",
    "                tqdm(query_or_dataset, desc=\"Sparse retrieval: \")\n",
    "            ):\n",
    "                tmp = {\n",
    "                    # Query와 해당 id를 반환합니다.\n",
    "                    \"question\": example[\"question\"],\n",
    "                    \"id\": example[\"id\"],\n",
    "                    # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "                    \"context\": \" \".join(\n",
    "                        [self.contexts[pid] for pid in doc_indices[idx]]\n",
    "                    ),\n",
    "                }\n",
    "                if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "                    # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "                    tmp[\"original_context\"] = example[\"context\"]\n",
    "                    tmp[\"answers\"] = example[\"answers\"]\n",
    "                total.append(tmp)\n",
    "\n",
    "            return pd.DataFrame(total)\n",
    "\n",
    "    def get_relevant_doc_faiss(\n",
    "        self, query: str, k: Optional[int] = 1 # for TF-IDF\n",
    "    ) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (str):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "\n",
    "        query_vec = self.tfidfv.transform([query])\n",
    "        assert (\n",
    "            np.sum(query_vec) != 0\n",
    "        ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "        q_emb = query_vec.toarray().astype(np.float32)\n",
    "        with timer(\"query faiss search\"):\n",
    "            D, I = self.indexer.search(q_emb, k)\n",
    "\n",
    "        return D.tolist()[0], I.tolist()[0]\n",
    "\n",
    "    def get_relevant_doc_bulk_faiss(\n",
    "        self, queries: List, k: Optional[int] = 1 # for TF-IDF\n",
    "    ) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            queries (List):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "\n",
    "        query_vecs = self.tfidfv.transform(queries)\n",
    "        assert (\n",
    "            np.sum(query_vecs) != 0\n",
    "        ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "        q_embs = query_vecs.toarray().astype(np.float32)\n",
    "        D, I = self.indexer.search(q_embs, k)\n",
    "\n",
    "        return D.tolist(), I.tolist()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"\")\n",
    "    parser.add_argument(\n",
    "        \"--dataset_name\", metavar=\"../data/train_dataset\", type=str, help=\"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        metavar=\"bert-base-multilingual-cased\",\n",
    "        type=str,\n",
    "        help=\"\",\n",
    "    )\n",
    "    parser.add_argument(\"--data_path\", metavar=\"../data\", type=str, help=\"\")\n",
    "    parser.add_argument(\n",
    "        \"--context_path\", metavar=\"wikipedia_documents\", type=str, help=\"\"\n",
    "    )\n",
    "    parser.add_argument(\"--use_faiss\", metavar=False, type=bool, default=False, help=\"\")\n",
    "    parser.add_argument(\"--emb_type\", metavar='bm25', type=str, help=\"\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Test sparse\n",
    "    org_dataset = load_from_disk(args.dataset_name)\n",
    "    full_ds = concatenate_datasets(\n",
    "        [\n",
    "            org_dataset[\"train\"].flatten_indices(),\n",
    "            org_dataset[\"validation\"].flatten_indices(),\n",
    "        ]\n",
    "    )  # train dev 를 합친 4192 개 질문에 대해 모두 테스트\n",
    "    print(\"*\" * 40, \"query dataset\", \"*\" * 40)\n",
    "    print(full_ds)\n",
    "    \n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=False,)\n",
    "    print(\"#\" * 100)\n",
    "    retriever = SparseRetrieval(\n",
    "        tokenize_fn=tokenizer.tokenize,\n",
    "        data_path=args.data_path,\n",
    "        context_path=args.context_path,\n",
    "        emb_type = args.emb_type,\n",
    "    )\n",
    "    print(\"#\" * 100)\n",
    "    ###\n",
    "    retriever.get_sparse_embedding()\n",
    "    retriever.build_faiss()\n",
    "    ###\n",
    "\n",
    "    query = \"대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?\"\n",
    "\n",
    "    if args.use_faiss:\n",
    "\n",
    "        # test single query\n",
    "        with timer(\"single query by faiss\"):\n",
    "            scores, indices = retriever.retrieve_faiss(query)\n",
    "\n",
    "        # test bulk\n",
    "        with timer(\"bulk query by exhaustive search\"):\n",
    "            df = retriever.retrieve_faiss(full_ds)\n",
    "            df[\"correct\"] = df[\"original_context\"] == df[\"context\"]\n",
    "\n",
    "            print(\"correct retrieval result by faiss\", df[\"correct\"].sum() / len(df))\n",
    "\n",
    "    else:\n",
    "        with timer(\"single query by exhaustive search\"):\n",
    "            scores, indices = retriever.retrieve(query)\n",
    "\n",
    "        with timer(\"bulk query by exhaustive search\"):\n",
    "            df = retriever.retrieve(full_ds)\n",
    "            df[\"correct\"] = df[\"original_context\"] == df[\"context\"]\n",
    "            print(\n",
    "                \"correct retrieval result by exhaustive search\",\n",
    "                df[\"correct\"].sum() / len(df),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25: #(Retrieval):\n",
    "    def __init__(\n",
    "        self, tokenize_fn,\n",
    "        data_path: Optional[str] = \"../data/\", \n",
    "        context_path: Optional[str] = \"wikipedia_documents.json\"\n",
    "    ):\n",
    "        super().__init__(tokenize_fn, data_path, context_path)\n",
    "        self.bm25 = None\n",
    "    def get_sparse_embedding(self):\n",
    "        with timer(\"bm25 building\"):\n",
    "            self.bm25 = BM25Okapi(self.contexts, tokenizer=self.tokenize_fn) \n",
    "        \n",
    "    def get_relevant_doc(self, query: str, k: Optional[int] = 1) -> Tuple[List, List]:\n",
    "        with timer(\"transform\"):\n",
    "            tokenized_query = self.tokenize_fn(query)\n",
    "        with timer(\"query ex search\"):\n",
    "            result = self.bm25.get_scores(tokenized_query)\n",
    "        sorted_result = np.argsort(result)[::-1]\n",
    "        doc_score = result[sorted_result].tolist()[:k]\n",
    "        doc_indices = sorted_result.tolist()[:k]\n",
    "        return doc_score, doc_indices\n",
    "\n",
    "    def get_relevant_doc_bulk(\n",
    "        self, queries: List, k: Optional[int] = 1\n",
    "    ) -> Tuple[List, List]:\n",
    "        with timer(\"transform\"):\n",
    "            tokenized_queris = [self.tokenize_fn(query) for query in queries]\n",
    "        with timer(\"query ex search\"):\n",
    "            result = np.array([self.bm25.get_scores(tokenized_query) for tokenized_query in tqdm(tokenized_queris)])\n",
    "        doc_scores = []\n",
    "        doc_indices = []\n",
    "        for i in range(result.shape[0]):\n",
    "            sorted_result = np.argsort(result[i, :])[::-1]\n",
    "            doc_scores.append(result[i, :][sorted_result].tolist()[:k])\n",
    "            doc_indices.append(sorted_result.tolist()[:k])\n",
    "        return doc_scores, doc_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boost0",
   "language": "python",
   "name": "boost0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
