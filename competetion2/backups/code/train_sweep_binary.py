import pickle as pickle
import os
import pandas as pd
import torch
import sklearn
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer
from load_data import *


## wandb & sweep ì¶”ê°€
import wandb

import argparse

import os
import random
import numpy as np

## seed ì¶”ê°€
def seed_everything(seed: int = 42, contain_cuda: bool = False):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    print(f"Seed set as {seed}")


##ì´ë¶€ë¶„ ìˆ˜ì •í•¨(binary)
def klue_re_micro_f1(preds, labels):
    """KLUE-RE micro f1"""
    label_list = ['no_relation', "else"]
    label_indices = list(range(len(label_list)))
    return sklearn.metrics.f1_score(labels, preds, average="micro", labels=label_indices) * 100.0


def compute_metrics(pred):
  """ validationì„ ìœ„í•œ metrics function """
  labels = pred.label_ids
  preds = pred.predictions.argmax(-1)
  probs = pred.predictions

  # calculate accuracy using sklearn's function
  f1 = klue_re_micro_f1(preds, labels)
  acc = accuracy_score(labels, preds) # ë¦¬ë”ë³´ë“œ í‰ê°€ì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

  return {
      'micro f1 score': f1,
      'accuracy': acc,
  }

##ì´ë¶€ë¶„ ìˆ˜ì •í•¨(binary)
def label_to_num(label):
  num_label = []
  for v in label:
    if v =="no_relation":
      num_label.append(0)
    else:
      num_label.append(1)  
  return num_label

# def train():
#   # load model and tokenizer
#   # MODEL_NAME = "bert-base-uncased"
#   # MODEL_NAME = "klue/bert-base"
#   # MODEL_NAME = "klue/roberta-base"
#   MODEL_NAME = "klue/roberta-large"
#   # MODEL_NAME = "monologg/koelectra-base-v3-discriminator"
#   # MODEL_NAME = "monologg/kobert"
#   # MODEL_NAME = "kykim/t5-kor-small" #Tokenizer ì–´ì©Œêµ¬ì €ì©Œêµ¬ ë¬¸ì œ ëœ¸
#   # MODEL_NAME = "kykim/gpt3-kor-small_based_on_gpt2" #TypeError ëœ¸
#   # MODEL_NAME = "kykim/funnel-kor-base"
#   # MODEL_NAME = "kykim/electra-kor-base"
#   # MODEL_NAME = ""
#   # MODEL_NAME = ""

  





#   tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

#   # load dataset
#   train_dataset = load_data("../dataset/train/train.csv")
#   # dev_dataset = load_data("../dataset/train/dev.csv") # validationìš© ë°ì´í„°ëŠ” ë”°ë¡œ ë§Œë“œì…”ì•¼ í•©ë‹ˆë‹¤.

#   train_label = label_to_num(train_dataset['label'].values)
#   # dev_label = label_to_num(dev_dataset['label'].values)

#   # tokenizing dataset
#   tokenized_train = tokenized_dataset(train_dataset, tokenizer)
#   # tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)

#   # make dataset for pytorch.
#   RE_train_dataset = RE_Dataset(tokenized_train, train_label)
#   # RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)

#   device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

#   print(device)
#   # setting model hyperparameter
#   model_config =  AutoConfig.from_pretrained(MODEL_NAME)
#   model_config.num_labels = 30 ##ë””í´íŠ¸ ê°’ìœ¼ë¡œ 2ë¡œ ì„¤ì • ë˜ì–´ ìˆë‹¤

#   model =  AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config)
#   ##<ë‚´ ì£¼ì„>
#   ##ë§Œì•½ ë¶ˆëŸ¬ì˜¬ë•Œ conifgë¥¼ ì§ì ‘ ìš°ë¦¬ê°€ ì…ë ¥í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ë©´ MODEL_NAMEìœ¼ë¡œ
#   ##í—ˆê¹…í˜ì´ìŠ¤ íƒ€ê³  ë“¤ì–´ê°€ì„œ ê±°ê¸°ì— ìˆëŠ” model_config íŒŒì¼ì„ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¨ë‹¤.
#   ##ë§Œì•½ ìš°ë¦¬ê°€ ì§ì ‘ ì…ë ¥í•´ì£¼ë©´ ìš°ë¦¬ê°€ ì…ë ¥í•´ì¤€ê±¸ ì‚¬ìš©í•œë‹¤!
  
#   print(model.config)
#   model.parameters
#   model.to(device)


  
#   # ì‚¬ìš©í•œ option ì™¸ì—ë„ ë‹¤ì–‘í•œ optionë“¤ì´ ìˆìŠµë‹ˆë‹¤.
#   # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments ì°¸ê³ í•´ì£¼ì„¸ìš”.
#   training_args = TrainingArguments(
#     output_dir='./results',          # output directory
#     save_total_limit=5,              # number of total save model.
#     save_steps=500,                 # model saving step.
#     num_train_epochs=1,              # total number of training epochs
#     learning_rate=5e-5,               # learning_rate
#     per_device_train_batch_size=16,  # batch size per device during training
#     per_device_eval_batch_size=16,   # batch size for evaluation
#     warmup_steps=500,                # number of warmup steps for learning rate scheduler
#     weight_decay=0.01,               # strength of weight decay
#     logging_dir='./logs',            # directory for storing logs
#     logging_steps=100,              # log saving step.
#     evaluation_strategy='steps', # evaluation strategy to adopt during training
#                                 # `no`: No evaluation during training.
#                                 # `steps`: Evaluate every `eval_steps`.
#                                 # `epoch`: Evaluate every end of epoch.
#     eval_steps = 500,            # evaluation step.
#     load_best_model_at_end = True ##ì´ê²Œ ì¤‘ìš”í•œ ì—­í• ì„ í•´ì£¼ëŠ” ê²ƒ ê°™ë‹¤! ## ì°¾ì•„ë³´ê¸°!!!
#   )
#   trainer = Trainer(
#     model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
#     args=training_args,                  # training arguments, defined above
#     train_dataset=RE_train_dataset,         # training dataset
#     eval_dataset=RE_train_dataset,             # evaluation dataset
#     compute_metrics=compute_metrics         # define metrics function
#   )

#   # train model
#   trainer.train()
#   model.save_pretrained('./best_model')



# def main():
#   train()


if __name__ == '__main__':


  # í•˜ì´í¼ íŒŒë¼ë¯¸í„° ë“± ê°ì¢… ì„¤ì •ê°’ì„ ì…ë ¥ë°›ìŠµë‹ˆë‹¤
  # í„°ë¯¸ë„ ì‹¤í–‰ ì˜ˆì‹œ : python3 run.py --batch_size=64 ...
  # ì‹¤í–‰ ì‹œ '--batch_size=64' ê°™ì€ ì¸ìë¥¼ ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ default ê°’ì´ ê¸°ë³¸ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤
  parser = argparse.ArgumentParser()
  parser.add_argument('--seed', default=42, type=int)
  parser.add_argument('--MODEL_NAME', default='klue/roberta-base', type=str)
  ## ëŒ€ë¬¸ìì„ì— ì£¼ì˜!!!(MODEL_NAME)

  # args = parser.parse_args(args=[]) ##ì´ê±°ë•Œë¬¸ì— ì•ˆë˜ì—ˆë˜ ê²ƒ
  args = parser.parse_args() #ì´ë ‡ê²Œ í•˜ë‹ˆê¹Œ ëœë‹¤

  seed_everything(args.seed)

  # method
  sweep_config = {
      'method': 'grid' # random, grid, bayes
  }


  # hyperparameters
  parameters_dict = {
      # 'epochs': {
      #     'value': [1,2]
      #     },
      'batch_size': {
          'values': [64]
          },
      # 'learning_rate': {
      #     'distribution': 'log_uniform_values',
      #     'min': 1e-5,
      #     'max': 1e-3
      # },
      # 'weight_decay': {
      #     'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]
      # },
  }

  sweep_config['parameters'] = parameters_dict

  sweep_id = wandb.sweep(
        sweep_config, # config ë”•ì…”ë„ˆë¦¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        project='angyungjabi_binary', # projectì˜ ì´ë¦„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        entity="bc_nlp_fp")


  def sweep_train(config=None):
    # load model and tokenizer
    # MODEL_NAME = "bert-base-uncased"
    # MODEL_NAME = "klue/bert-base"
    # MODEL_NAME = "klue/roberta-base"
    # MODEL_NAME = "klue/roberta-large"
    # MODEL_NAME = "monologg/koelectra-base-v3-discriminator"
    # MODEL_NAME = "monologg/kobert"
    # MODEL_NAME = "kykim/t5-kor-small" #Tokenizer ì–´ì©Œêµ¬ì €ì©Œêµ¬ ë¬¸ì œ ëœ¸
    # MODEL_NAME = "kykim/gpt3-kor-small_based_on_gpt2" #TypeError ëœ¸
    # MODEL_NAME = "kykim/funnel-kor-base"
    # MODEL_NAME = "kykim/electra-kor-base"
    # MODEL_NAME = ""
    # MODEL_NAME = ""

    tokenizer = AutoTokenizer.from_pretrained(args.MODEL_NAME)

    # load dataset
    train_dataset = load_data("../dataset/train/train_binary.csv")
    dev_dataset = load_data("../dataset/train/dev_binary.csv") # validationìš© ë°ì´í„°ëŠ” ë”°ë¡œ ë§Œë“œì…”ì•¼ í•©ë‹ˆë‹¤.

    train_label = label_to_num(train_dataset['label'].values)
    dev_label = label_to_num(dev_dataset['label'].values)

    # tokenizing dataset
    tokenized_train = tokenized_dataset(train_dataset, tokenizer)
    tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)

    # make dataset for pytorch.
    RE_train_dataset = RE_Dataset(tokenized_train, train_label)
    RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

    print(device)
    # setting model hyperparameter
    model_config =  AutoConfig.from_pretrained(args.MODEL_NAME)
    
    ##ì´ë¶€ë¶„ ìˆ˜ì •í•¨(binary)
    model_config.num_labels = 2 ##ë””í´íŠ¸ ê°’ìœ¼ë¡œ 2ë¡œ ì„¤ì • ë˜ì–´ ìˆë‹¤

    model =  AutoModelForSequenceClassification.from_pretrained(args.MODEL_NAME, config=model_config)
    ##<ë‚´ ì£¼ì„>
    ##ë§Œì•½ ë¶ˆëŸ¬ì˜¬ë•Œ conifgë¥¼ ì§ì ‘ ìš°ë¦¬ê°€ ì…ë ¥í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ë©´ MODEL_NAMEìœ¼ë¡œ
    ##í—ˆê¹…í˜ì´ìŠ¤ íƒ€ê³  ë“¤ì–´ê°€ì„œ ê±°ê¸°ì— ìˆëŠ” model_config íŒŒì¼ì„ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¨ë‹¤.
    ##ë§Œì•½ ìš°ë¦¬ê°€ ì§ì ‘ ì…ë ¥í•´ì£¼ë©´ ìš°ë¦¬ê°€ ì…ë ¥í•´ì¤€ê±¸ ì‚¬ìš©í•œë‹¤!
    
    print(model.config)
    model.parameters
    model.to(device)

    #####ì—¬ê¸°ë¶€í„° ìˆ˜ì •
    wandb.init(config=config)

    # set sweep configuration
    w_config = wandb.config

    ## wandbì— ì €ì¥ë˜ëŠ” ì´ë¦„ ì§€ì •
    wandb.run.name = f'{"-".join(args.MODEL_NAME.split("/"))}_binary_batch{w_config.batch_size}' ##ì´ê±° ì´ë¦„ì— / ë“¤ì–´ê°€ ìˆì–´ì„œ ë¶ˆì•ˆí•˜ê¸´ í•œë°...
    

    # ì‚¬ìš©í•œ option ì™¸ì—ë„ ë‹¤ì–‘í•œ optionë“¤ì´ ìˆìŠµë‹ˆë‹¤.
    # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments ì°¸ê³ í•´ì£¼ì„¸ìš”.
    training_args = TrainingArguments(
      # output_dir='./results',          # output directory
      output_dir = f'./results/{"-".join(args.MODEL_NAME.split("/"))}/binary/batch{w_config.batch_size}',

      ##ìƒˆë¡­ê²Œ ì¶”ê°€
      report_to='wandb',  # Turn on Weights & Biases logging
      # run_name=f"model_{w_config.batch_size}",  # name of the W&B run (optional)
      
      save_total_limit=5,              # number of total save model.
      save_steps=500,                 # model saving step.
      num_train_epochs=10,              # total number of training epochs
      learning_rate=5e-5,               # learning_rate
      per_device_train_batch_size= w_config.batch_size,  # batch size per device during training
      per_device_eval_batch_size=w_config.batch_size,   # batch size for evaluation
      warmup_steps=500,                # number of warmup steps for learning rate scheduler
      weight_decay=0.01,               # strength of weight decay
      logging_dir='./logs',            # directory for storing logs
      logging_steps=100,              # log saving step.
      evaluation_strategy='steps', # evaluation strategy to adopt during training
                                  # `no`: No evaluation during training.
                                  # `steps`: Evaluate every `eval_steps`.
                                  # `epoch`: Evaluate every end of epoch.
      eval_steps = 500,            # evaluation step.
      load_best_model_at_end = True ##ì´ê²Œ ì¤‘ìš”í•œ ì—­í• ì„ í•´ì£¼ëŠ” ê²ƒ ê°™ë‹¤! ## ì°¾ì•„ë³´ê¸°!!!
    )
    trainer = Trainer(
      model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
      args=training_args,                  # training arguments, defined above
      train_dataset=RE_train_dataset,         # training dataset
      eval_dataset=RE_dev_dataset,             # evaluation dataset
      compute_metrics=compute_metrics         # define metrics function
    )

    print(args)
    print("woojin jjjjang")

    # train model
    trainer.train()
    
    #model.save_pretrained('./best_model')
    model.save_pretrained(f'./best_model/{"-".join(args.MODEL_NAME.split("/"))}/binary/batch{w_config.batch_size}')




  wandb.agent(
        sweep_id,      # sweepì˜ ì •ë³´ë¥¼ ì…ë ¥í•˜ê³ 
        function=sweep_train,   # trainì´ë¼ëŠ” ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ì½”ë“œë¥¼
        count=10               # ì´ 10íšŒ ì‹¤í–‰í•´ë´…ë‹ˆë‹¤.(gridì˜ ê²½ìš° 10ë²ˆì´ ì•ˆë˜ì–´ë„ ë‹¤í•˜ë©´ ë©ˆì¶”ëŠ” ë“¯)
    )